ELK Stack:

#Lab One: ElasticSearch
	#Installation:
	ELASTICSEARCH:
	#	First- please ensure that you have java version 7 or higher installed on your system. Go to the command 
	#	line and type:
			java –version
	# The version MUST be higher than java 7. If it is not please install latest JAVA version here:
	#	https://java.com/en/download/help/mac_install.xml
	
	#MAC OSX: 
	brew install elasticsearch

	#Now that we have elasticsearch installed on our system let's get the node started (we are on a single node system here):

	brew services start elasticsearch

	#(alternatively we can go to the elasticsearch install location and do 
	cd /usr/local/Cellar/elasticsearch/5.1.2/bin 
	./elasticsearch
	#Which will bring up our cluster with verbose language- and not start it on startup.

	#Now we want to check on the health of our cluster. Please run the following curl command in your
	#command line:
	curl -XGET 'localhost:9200/_cluster/health?pretty'

	#What we should see here is "Cluster health status" of green. This is a useful command that we 
	#will come back to again and again. We can also check status' of indexes and shards.
	#Please note the default five shards per index. 

#Lab 2: CRUD
	#Obviously at this point we are interested in Creating, Retrieving, Updating, Deleting data
	#So let's create a document. We do this by utilizing the PUT command for a JSON doc.
	#As I work (sort of) in the film industry I am going to utilize a film for our document. 
	#That film will be star wars. 
	#When we "put" a film into the elasticsearch index we name the index (films), the type (movie)
	#and then the unique identifier (1)

	curl -XPUT "http://localhost:9200/films/movie/1" -d' { "title": "Star Wars", "director": "George Lucas", "year": 1977 }'

	#So we've identified this as document "1". Let's retrieve it:
	curl -XGET "http://localhost:9200/films/movie/1?pretty"

	#Notice that I've added a "pretty" option here. This parameter allows us to see the data in a #nicely formatted way.
	#Now- we've created an index- let's look at some metadata about it. We will do this with the #"_mapping" parameter. Think of this as the "Describe" function in SQL
	
	curl -XGET "http://localhost:9200/films/_mapping?pretty"
	
	#And what if we want the equivalent of "SELECT *"?

	curl -XGET "http://localhost:9200/_all/_mapping?pretty"

	#And if you want *just* the data and no metadata- utilize the "_source" command:

	curl -XGET "http://localhost:9200/films/movie/1/_source?pretty"

	#Now- with Star Wars we identified a document number (1). What if we want the equivalent from 
	#relational databases of an incremental non-null unique identifier? 
	#Instead of using "PUT" we would use "POST" AND we would do something like this:

	curl -XPOST "http://localhost:9200/films/movie/ -d' {"title":"Empire Strikes Back", "Director": "Irvin Kershner", "Year": 1980 }'

	#Now that that is in there let's look at all of our current docs:
	curl -XGET "http://localhost:9200/films/_search?pretty"
	#Do you see the document ID that was auto-generated by elasticsearch?

	#Now let's talk about updating your documents. Here our issue is that unfortunately (as per our #slide) once a document is flushed to a disk it is immutable (written once read often). SO this
	#means that in order to update a document, Elasticsearch runs a GET request, modifies the _source 
	#field, removes the old document, and indexes a new document with the updated content. For 
	#instance- here we want to update our FILM (Star wars) and add in a "genres" list.
	#We must update the entire document as so:

	curl -XPUT "http://localhost:9200/films/movie/1" -d' { "title": "Star Wars", "director": "George Lucas", "year": 1977, "genres": ["Sci-Fi", "Adventure"]}'

	#Did you see the _version: value returned? 
	#We also have the UPSERT here that will update a doc for, for example, a counter. Let's create 
	#a quick index to demonstrate:

	curl -XPUT 'http://localhost:9200/test/type1/1' -d' { "counter" : 1, "tags" : ["red"] }'

	#So here we have a "Counter" field and a "Tags" field. 
	#We have a program called ctx that allows us to run scripts to update- for example:

	curl -XPOST 'localhost:9200/test/type1/1/_update?pretty' -d' { "script" : {
        "inline": "ctx._source.counter += params.count",
        "lang": "painless",
        "params" : {
            "count" : 4
        			}
    			}
			}'

	#The above script will add 4 to the counter of test/type1 document each time it is run.
	#This can also be used to update tags as such:

	curl -XPOST 'localhost:9200/test/type1/1/_update?pretty' -d'
		{
    "script" : {
        "inline": "ctx._source.tags.add(params.tag)",
        "lang": "painless",
        "params" : {
            "tag" : "blue"
		        }
		    }
		}
		'

	#And finally- you can do a partial upsert by setting the parameter "doc_as_upsert" to TRUE as 
	#here:

	curl -XPOST 'localhost:9200/test/type1/1/_update?pretty' -d' { "doc" : {
        "name" : "new_name" },
    "doc_as_upsert" : true }'

    #Now let's talk about deleting- which is pretty simple: 
    curl -XDELETE 'http://localhost:9200/test/type1/1'

    #It is possible to have multiple versions of a doc out there (remember the version parameter)?
    #For example:

	curl -XPUT "http://localhost:9200/films/movie/2" -d '{ "title": "Return of the Jedi", "director": "Richard Marquand", "year": 1983, "genres": ["Sci-Fi", "Adventure"], "rank": "the best" }'

	curl -XPUT "http://localhost:9200/films/movie/2" -d '{ "title": "Return of the Jedi", "director": "Richard Marquand", "year": 1983, "genres": ["Sci-Fi", "Adventure"], "rank": "the worst" }'

	curl -XDELETE 'http://localhost:9200/films/movie/2?version=1'

	#Bulk updates: I have included a file called "shakespeare.json" which includes the complete works
	#of Billy Shakespeare. Download it through git and then let's bulk insert it into our
	#single node cluster (from within the directory)

	curl -XPUT localhost:9200/_bulk --data-binary @shakespeare.json

	#Now let's run to check out health again: 
	curl 'localhost:9200/_cat/indices?v'

#Lab 3: Search API:
	#Now that shakespeare is loaded let's get some data back. Run this query:

	curl -XGET "localhost:9200/shakespeare/_search?q=*&sort=line_id:asc&pretty"

	#Breaking it down: q=* is search all docs, sort is obvious:
	#took – time in milliseconds for Elasticsearch to execute the search
	#timed_out – tells us if the search timed out or not
	#_shards – tells us how many shards were searched, as well as a count of the successful/failed #searched shards
	#hits – search results
	#hits.total – total number of documents matching our search criteria
	#hits.hits – actual array of search results (defaults to first 10 documents)
	#sort - sort key for results (missing if sorting by score)
	#_score and max_score - ignore these fields for now

	#We can also do this with the "request body" method:

curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
{
  "query": { "match_all": {} },
  "sort": [
    { "line_id": "asc" }
  ]
}

	#"match all" means "search all documents in the index"
	#We also have parameters like "size" which is the equivalent of "limit" (defaults to 10)
	#We can also do "from, to":

	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
	  "query": { "match_all": {} },
	  "size": 1
	}
	'

	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
	  "query": { "match_all": {} },
	  "from": 10,
	  "size": 10
	}
	'

	#We can also limit the sections we get back. For example- if we only want the phrase and 
	#who said it:

	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
	  "query": { "match_all": {} },
	  "_source": ["speaker", "text_entry"]
	}
	'

	#We also have the "match" query which, unlike "match all" looks at a basic fielded search query.
	#For example- let's find Romeo & Juliet

	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
  		"query": { "match": { "speaker": "ROMEO" } }
	}
	'


	#Quick note on "match"- as search functionality it will do an "or", so:
	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
  		"query": { "match": { "speaker": "KING JULIET" } },
  		"size": 1000 
	}
	'
	#This is the equivalent of "return King OR Juliet as speaker". To match a phrase you might want:
	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
	  "query": { "match_phrase": { "text_entry": "To be or not to be" } }
	}
	'

	#Now- to speak of the "bool" clause we must remember the difference between "and" and "or":

	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
	  "query": {
	    "bool": {
	      "must": [
	        { "match_phrase": { "speaker": "Romeo" } },
	        { "match_phrase": { "text_entry": "But, soft! what light through yonder window breaks?" } }
			      ]
			    }
			  }
			}
		'

	#And for the "or" equivalent

	curl -XGET 'localhost:9200/shakespeare/_search?pretty' -d'
	{
	  "query": {
	    "bool": {
	      "should": [
	        { "match": { "speaker": "Juliet" } },
	        { "match": { "text_entry": "Once more unto the breach" } }
	      ]
	    }
	  }
	}
	'
#There are also "must not" clauses that go into the bool.
#The "type" query is also a broad query that we can utilize to look at "type". 

#It is also possible to look for greater or less than for numeric values (this is an 
#example from your practice data that you can find in the same folder). This is a 
#query that will return balances in bank accounts between 20000 and 30000 dollars.
#Notice the use of "range" which is similiar to "between" in SQL Where clauses.
		curl -XGET 'localhost:9200/bank/_search?pretty' -d'
		{
		  "query": {
		    "bool": {
		      "must": { "match_all": {} },
		      "filter": {
		        "range": {
		          "balance": {
		            "gte": 20000,
		            "lte": 30000
		          }
		        }
		      }
		    }
		  }
		}
		'



#Now let's use filtering to get to a specific speach. Let's do HAMLET'S "to be or not to be?"
#Please take 30 minutes and create your query. 
#Please also find the following: 
#Bank accounts with over $100,000 
#Everyone with bank accounts from mill lane
#Everyone from Washington State
#All females from Pennsylvania over 35
#List top ten account numbers in order according to how much they have in the bank

#LAB 4: Analyzers:
	#Download and run an sh on tweet_test_data.sh to load data then run this command:
	curl -XGET 'localhost:9200/_cat/indices?pretty'
	#You should see a list of your active indices:
	#yellow open gb          C4sQVpwMRqiUR2ybnUV09Q 5 1      7 0 20.1kb 20.1kb
	#yellow open films       SpuuDsoNSBucLoY86xKXqg 5 1      4 0 19.7kb 19.7kb
	#yellow open us          cEF80Wj2QFqXd0i42F6taw 5 1      7 0 20.1kb 20.1kb
	#yellow open test        JwcQW-3LRDiW_TSnGTgUIQ 5 1      0 0   679b   679b
	#yellow open shakespeare 7J7wVenvSg-5S9oKeuEcZw 5 1 111394 0 28.7mb 28.7mb


	#Now let's run a quick search:
	curl -XGET 'localhost:9200/us,gb/_search?pretty'

	#Now that we see what the data looks like let's run a second group of searches:

	curl -XGET 'localhost:9200/us,gb/_search?q=2014&pretty'             # 5 results'
	curl -XGET 'localhost:9200/us,gb/_search?q=2014-09-15&pretty'       # 12 results'
	curl -XGET 'localhost:9200/us,gb/_search?q=date:2014-09-15&pretty'   # 1  result'
	curl -XGET 'localhost:9200/us,gb/_search?q=date:2014&pretty'         # 0  results'

	#Why does querying the _all field for the full date return all tweets, and querying the date #field for just the year return no results? Why do our results differ when searching within the #_all field or the date field?

	#Presumably, it is because the way our data has been indexed in the _all field is different from #how it has been indexed in the date field. So let’s take a look at how Elasticsearch has #interpreted our document structure, by requesting the mapping (or schema definition) for the #tweet type in the gb indexed

	curl -XGET 'localhost:9200/gb/_mapping/tweet?pretty'

	#Now let's look at how a certain phrase is analyzed:
	#The 2 QUICK Brown-Foxes jumped over the lazy dog\u0027s bone.
	curl -XPOST 'localhost:9200/_analyze?pretty' -d' {
	  "analyzer": "standard",
	  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog\u0027s bone."
	}'

	#Now- let's create our own customized analyzer that limits the tokenization to five chars
	#in length:
	curl -XPUT 'localhost:9200/my_index?pretty' -d'
	{
	  "settings": {
	    "analysis": {
	      "analyzer": {
	        "my_english_analyzer": {
	          "type": "standard",
	          "max_token_length": 5,
	          "stopwords": "_english_"
	        }
	      }
	    }
	  }
	}
	'
	curl -XPOST 'localhost:9200/my_index/_analyze?pretty' -d'
	{
	  "analyzer": "my_english_analyzer",
	  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog\u0027s bone."
	}
	'

	#Do you see the difference with words over five characters?
	
#Lab 6: Mapping Lab

	#So we have established that mapping can *sort of* define schemas. Remember the exception
	#that any fields with the same name in the same index MUST have the same mapping

	#So let's create another index:
	curl -XPUT 'localhost:9200/my_index?pretty' -d'
	{
	  "mappings": {
	    "user": { 
	      "_all":       { "enabled": false  }, 
	      "properties": { 
	        "title":    { "type": "text"  }, 
	        "name":     { "type": "text"  }, 
	        "age":      { "type": "integer" }  
	      }
	    },
	    "blogpost": { 
	      "_all":       { "enabled": false  }, 
	      "properties": { 
	        "title":    { "type": "text"  }, 
	        "body":     { "type": "text"  }, 
	        "user_id":  {
	          "type":   "keyword" 
	        },
	        "created":  {
	          "type":   "date", 
	          "format": "strict_date_optional_time||epoch_millis"
	        }
	      }
	    }
	  }
	}
	'
	#Now- the _all value- when enabled, basically makes a large analyzed terms list- so this:
		{
		  "first_name":    "John",
		  "last_name":     "Smith",
		  "date_of_birth": "1970-10-24"
		}
	#The analysis with "_all" enabled creates these terms:
	#[ "john", "smith", "1970", "10", "24" ]

	#indexedit
	#The index attribute controls how the string will be indexed. It can contain one of three #values:

	#analyzed
	#First analyze the string and then index it. In other words, index this field as full text.
	#not_analyzed
	#Index this field, so it is searchable, but index the value exactly as specified. Do not #analyze it.
	#no
	#Don’t index this field at all. This field will not be searchable.

#Lab 7: Querying

	#Let's start by querying our shakespeare database with a match_all query (think of it as SELECT #*):
	curl -XGET 'localhost:9200/_search?pretty' -d'
	{
	    "query": {
	        "match_all": {}
   				 }
	}'


	#Now let's "match" a specific query. If we want multiple matches we can run this:

	curl -XGET 'localhost:9200/_search?pretty' -d'
	{
	    "query": {
	        "match": { "play_name": "Henry IV"}},
			"match": { "speech_number": 2}},
			"match": { "speaker": "Westmoreland"}},
			"match": { "text_entry": "Leading the men of Herefordshire to fight"}},
	}'

	{
    "match" : {
        "message" : {
            "query" : "to be or not to be",
            "operator" : "and",
            "zero_terms_query": "all"
        }
    }
}

curl -XGET 'localhost:9200/_search?pretty' -d'
{
    "query": {
        "match" : {
            "message" : {
                "query" : "to be or not to be",
                "zero_terms_query": "all"
            }
        }
    }
}
'

